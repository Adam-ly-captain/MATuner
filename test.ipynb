{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 142       |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 18.3      |\n",
      "|    critic_loss     | 0.052     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 600       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.47e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 119       |\n",
      "|    time_elapsed    | 13        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 45.2      |\n",
      "|    critic_loss     | 0.0651    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1400      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.25e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 112       |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 60.8      |\n",
      "|    critic_loss     | 0.119     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2200      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.21e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 109       |\n",
      "|    time_elapsed    | 29        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 70.4      |\n",
      "|    critic_loss     | 0.75      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3000      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.15e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 107       |\n",
      "|    time_elapsed    | 37        |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 78.2      |\n",
      "|    critic_loss     | 0.664     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3800      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 105       |\n",
      "|    time_elapsed    | 45        |\n",
      "|    total_timesteps | 4800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 82.1      |\n",
      "|    critic_loss     | 1.24      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 4600      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -940     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76       |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5400     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -845     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 105      |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 69.5     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6200     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -765     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 104      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.9     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -707     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 104      |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 54.6     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -657     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 50       |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8600     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -610     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.5     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9400     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 22\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     24\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\stable_baselines3\\common\\policies.py:355\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import DDPG\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "# 创建DDPG模型\n",
    "model = DDPG('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# 训练模型\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"ddpg_pendulum\")\n",
    "\n",
    "# 加载模型\n",
    "loaded_model = DDPG.load(\"ddpg_pendulum\")\n",
    "\n",
    "# 测试模型\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = loaded_model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    if dones:\n",
    "        obs = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解向量 x: [ 8.5 10.  10.  10. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "gama = 0.9\n",
    "# 定义系数矩阵 A 和常数向量 b\n",
    "A = np.array([[1, -0.5 * gama, -0.5 * gama, 0], [0, 1, 0, -gama], [0, 0, 1, -gama], [0, 0, 0, 1 - gama]])\n",
    "b = np.array([-0.5, 1, 1, 1])\n",
    "\n",
    "# 求解线性方程组\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"解向量 x:\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function v: [24.87603306 28.18181818 30.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义状态转移概率矩阵 P 和奖励向量 r\n",
    "P = np.array([[0.5, 0.5, 0.0],\n",
    "              [0.0, 0.5, 0.5],\n",
    "              [0.0, 0.0, 1.0]])\n",
    "\n",
    "r = np.array([1.0, 2.0, 3.0]).reshape(-1, 1)  # 转换成列向量\n",
    "\n",
    "# 定义折扣因子 gamma\n",
    "gamma = 0.9\n",
    "\n",
    "# 求解状态值函数 v\n",
    "I = np.eye(P.shape[0])\n",
    "v = np.linalg.inv(I - gamma * P).dot(r)\n",
    "\n",
    "print(\"Optimal value function v:\", v.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.999990879655444"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = 0\n",
    "v2 = 0\n",
    "lastv1 = -999\n",
    "lastv2 = -999\n",
    "while (abs(lastv1 - v1) > 0.000001 and abs(lastv2 - v2) > 0.000001):\n",
    "  lastv1 = v1\n",
    "  lastv2 = v2\n",
    "  v1 = -1 + 0.9 * v1\n",
    "  v2 = 0 + 0.9 * v1\n",
    "\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['left', 'left', 'down', 'left', 'down'],\n",
       " ['right', 'up', 'down', 'up', 'right'],\n",
       " ['left', 'right', 'down', 'left', 'down'],\n",
       " ['left', 'right', 'up', 'up', 'right'],\n",
       " ['up', 'right', 'up', 'down', 'right']]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define the maze environment\n",
    "maze = np.array([\n",
    "  [0, 0, 0, 0, 0],\n",
    "  [0, 1, 1, 1, 0],\n",
    "  [0, 1, 2, 1, 0],\n",
    "  [0, 1, 0, 1, 0],\n",
    "  [0, 1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Define the actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((maze.shape[0], maze.shape[1], len(actions)))\n",
    "\n",
    "# Define the hyperparameters\n",
    "alpha = 0.001  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # exploration rate\n",
    "\n",
    "# Define the Sarsa algorithm\n",
    "def sarsa_algorithm():\n",
    "  # Start at a random position in the maze\n",
    "  current_state = (np.random.randint(0, maze.shape[0]), np.random.randint(0, maze.shape[1]))\n",
    "  \n",
    "  # Choose an action using epsilon-greedy policy\n",
    "  # if np.random.uniform(0, 1) < epsilon:\n",
    "  #   action = np.random.choice(actions)\n",
    "  # else:\n",
    "  #   action = actions[np.argmax(q_table[current_state])]\n",
    "  action = np.random.choice(actions)\n",
    "  \n",
    "  episode = 0\n",
    "\n",
    "  # Repeat until reaching the goal state\n",
    "  # while maze[current_state] != 2:\n",
    "\n",
    "  while episode < 1000:\n",
    "    # Take the chosen action\n",
    "    if action == 'up':\n",
    "      next_state = (current_state[0] - 1, current_state[1])\n",
    "    elif action == 'down':\n",
    "      next_state = (current_state[0] + 1, current_state[1])\n",
    "    elif action == 'left':\n",
    "      next_state = (current_state[0], current_state[1] - 1)\n",
    "    elif action == 'right':\n",
    "      next_state = (current_state[0], current_state[1] + 1)\n",
    "    \n",
    "    reward = 0\n",
    "\n",
    "    if next_state[0] < 0 or next_state[0] >= maze.shape[0] or next_state[1] < 0 or next_state[1] >= maze.shape[1]:\n",
    "      next_state = current_state\n",
    "      reward = -1\n",
    "\n",
    "    if maze[next_state] == 1:\n",
    "      reward = -1\n",
    "    elif maze[next_state] == 2:\n",
    "      reward = 1\n",
    "\n",
    "    # Choose the next action using epsilon-greedy policy\n",
    "    # if np.random.uniform(0, 1) < epsilon:\n",
    "    #   next_action = np.random.choice(actions)\n",
    "    # else:\n",
    "    #   next_action = actions[np.argmax(q_table[next_state])]\n",
    "    # 找出所有最大值动作数组的下标\n",
    "    max_index = np.where(q_table[next_state] == np.max(q_table[next_state]))\n",
    "    # 随机选择一个最大值动作\n",
    "    next_action = actions[np.random.choice(max_index[0])]\n",
    "    \n",
    "    # Update the Q-table using the Sarsa update rule\n",
    "    q_table[current_state][actions.index(action)] += alpha * (\n",
    "      reward + gamma * q_table[next_state][actions.index(next_action)] - q_table[current_state][actions.index(action)]\n",
    "    )\n",
    "    \n",
    "    # Move to the next state and action\n",
    "    current_state = next_state\n",
    "    action = next_action\n",
    "\n",
    "    episode += 1\n",
    "  \n",
    "  return q_table\n",
    "\n",
    "# Run the Sarsa algorithm\n",
    "q_table = sarsa_algorithm()\n",
    "\n",
    "# Print the final Q-table\n",
    "# print(q_table)\n",
    "\n",
    "res_matrix = [['' for _ in range(maze.shape[1])] for _ in range(maze.shape[0])]\n",
    "for row in range(maze.shape[0]):\n",
    "  for col in range(maze.shape[1]):\n",
    "      max_index = np.where(q_table[row][col] == np.max(q_table[row][col]))\n",
    "      # 随机选择一个最大值动作\n",
    "      next_action = actions[np.random.choice(max_index[0])]\n",
    "      res_matrix[row][col] = next_action\n",
    "\n",
    "res_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6425112833001898"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "π(t) = [0.03448276 0.10837438 0.13300493 0.72413793]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_stationary_distribution(P, pi_0, num_steps):\n",
    "    pi = pi_0\n",
    "    for _ in range(num_steps):\n",
    "        pi = np.dot(pi, P)\n",
    "    return pi\n",
    "\n",
    "# 定义初始状态分布\n",
    "pi_0 = np.array([0.2, 0.4, 0.4, 0])\n",
    "\n",
    "# 定义状态转移概率矩阵\n",
    "P = np.array([[0.3, 0.1, 0.6, 0],\n",
    "              [0.1, 0.3, 0, 0.6],\n",
    "              [0.1, 0, 0.3, 0.6],\n",
    "              [0, 0.1, 0.1, 0.8]])\n",
    "\n",
    "# 求解π(t)\n",
    "num_steps = 100\n",
    "pi_t = compute_stationary_distribution(P, pi_0, num_steps)\n",
    "print(\"π(t) =\", pi_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值: [-0.04494897  0.44494897  1.          0.3       ]\n",
      "特征向量: [[ 0.81669314 -0.95373948 -0.5         0.69748583]\n",
      " [-0.40245351 -0.1974908  -0.5         0.69748583]\n",
      " [-0.40245351 -0.1974908  -0.5        -0.11624764]\n",
      " [ 0.09526102  0.11124643 -0.5        -0.11624764]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义一个矩阵\n",
    "A = np.array([[0.3, 0.1, 0.6, 0],\n",
    "              [0.1, 0.3, 0, 0.6],\n",
    "              [0.1, 0, 0.3, 0.6],\n",
    "              [0, 0.1, 0.1, 0.8]])\n",
    "\n",
    "# 求解特征值和特征向量\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# 打印特征值和特征向量\n",
    "print(\"特征值:\", eigenvalues)\n",
    "print(\"特征向量:\", eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [98], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m q_table\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Run the Q-Learning algorithm\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m q_table \u001b[38;5;241m=\u001b[39m q_learning_algorithm()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Print the final Q-table\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(q_table)\n",
      "Cell \u001b[1;32mIn [98], line 35\u001b[0m, in \u001b[0;36mq_learning_algorithm\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m max_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(q_table[row][col] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_table[row][col]))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 随机选择一个最大值动作\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m action \u001b[38;5;241m=\u001b[39m actions[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Take the chosen action\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the maze environment\n",
    "maze = np.array([\n",
    "  [0, 0, 0, 0, 0],\n",
    "  [0, 1, 1, 1, 0],\n",
    "  [0, 1, 2, 1, 0],\n",
    "  [0, 1, 0, 1, 0],\n",
    "  [0, 1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Define the actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((maze.shape[0], maze.shape[1], len(actions)))\n",
    "\n",
    "# Define the hyperparameters\n",
    "alpha = 0.01  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.7  # exploration rate\n",
    "\n",
    "# Define the Q-Learning algorithm\n",
    "def q_learning_algorithm():\n",
    "  # Start at a random position in the maze\n",
    "  current_state = (np.random.randint(0, maze.shape[0]), np.random.randint(0, maze.shape[1]))\n",
    "  \n",
    "  episode = 0\n",
    "\n",
    "  # Repeat until reaching the goal state\n",
    "  while maze[current_state] != 2:\n",
    "\n",
    "    max_index = np.where(q_table[row][col] == np.max(q_table[row][col]))\n",
    "    # 随机选择一个最大值动作\n",
    "    action = actions[np.random.choice(max_index[0])]\n",
    "    \n",
    "    # Take the chosen action\n",
    "    if action == 'up':\n",
    "      next_state = (current_state[0] - 1, current_state[1])\n",
    "    elif action == 'down':\n",
    "      next_state = (current_state[0] + 1, current_state[1])\n",
    "    elif action == 'left':\n",
    "      next_state = (current_state[0], current_state[1] - 1)\n",
    "    elif action == 'right':\n",
    "      next_state = (current_state[0], current_state[1] + 1)\n",
    "    \n",
    "    reward = 0\n",
    "\n",
    "    if next_state[0] < 0 or next_state[0] >= maze.shape[0] or next_state[1] < 0 or next_state[1] >= maze.shape[1]:\n",
    "      next_state = current_state\n",
    "      reward = -1\n",
    "\n",
    "    if maze[next_state] == 1:\n",
    "      reward = -1\n",
    "    elif maze[next_state] == 2:\n",
    "      reward = 1\n",
    "\n",
    "    # Update the Q-table using the Q-Learning update rule\n",
    "    q_table[current_state][actions.index(action)] += alpha * (\n",
    "      reward + gamma * np.max(q_table[next_state]) - q_table[current_state][actions.index(action)]\n",
    "    )\n",
    "    \n",
    "    # Move to the next state\n",
    "    current_state = next_state\n",
    "\n",
    "    episode += 1\n",
    "  \n",
    "  return q_table\n",
    "\n",
    "# Run the Q-Learning algorithm\n",
    "q_table = q_learning_algorithm()\n",
    "\n",
    "# Print the final Q-table\n",
    "print(q_table)\n",
    "\n",
    "res_matrix = [['' for _ in range(maze.shape[1])] for _ in range(maze.shape[0])]\n",
    "for row in range(maze.shape[0]):\n",
    "  for col in range(maze.shape[1]):\n",
    "      max_index = np.where(q_table[row][col] == np.max(q_table[row][col]))\n",
    "      # 随机选择一个最大值动作\n",
    "      next_action = actions[np.random.choice(max_index[0])]\n",
    "      res_matrix[row][col] = next_action\n",
    "\n",
    "res_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: tensor([[-0.0941, -0.0092, -0.1027]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义深度 Q 网络模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 定义输入数据\n",
    "input_data = torch.randn(1, 4)  # 例如，input_dim=4 表示输入特征维度为4\n",
    "\n",
    "# 创建深度 Q 网络模型\n",
    "input_dim = 4  # 输入特征维度\n",
    "output_dim = 3  # 输出动作空间大小\n",
    "model = DQN(input_dim, output_dim)\n",
    "\n",
    "# 输入数据并获取输出\n",
    "q_values = model(input_data)\n",
    "print(\"Q-values:\", q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 53.152938898879796\n",
      "Episode 1, Reward: 48.8535073458966\n",
      "Episode 2, Reward: 55.236282734970054\n",
      "Episode 3, Reward: 50.18536504124816\n",
      "Episode 4, Reward: 52.59105410960533\n",
      "Episode 5, Reward: 50.449244260789136\n",
      "Episode 6, Reward: 47.064814038946935\n",
      "Episode 7, Reward: 53.20842792290882\n",
      "Episode 8, Reward: 47.20591648286662\n",
      "Episode 9, Reward: 55.84312884452012\n",
      "Episode 10, Reward: 49.950081810273005\n",
      "Episode 11, Reward: 46.6183389212895\n",
      "Episode 12, Reward: 50.44816194270835\n",
      "Episode 13, Reward: 53.996252192061206\n",
      "Episode 14, Reward: 54.538464381081376\n",
      "Episode 15, Reward: 47.88550594086813\n",
      "Episode 16, Reward: 45.47614510290453\n",
      "Episode 17, Reward: 50.91584414963739\n",
      "Episode 18, Reward: 49.76009801314649\n",
      "Episode 19, Reward: 51.072129565266195\n",
      "Episode 20, Reward: 52.69576431436978\n",
      "Episode 21, Reward: 50.00910788492686\n",
      "Episode 22, Reward: 50.46425338532322\n",
      "Episode 23, Reward: 45.86531656762906\n",
      "Episode 24, Reward: 51.06868210575136\n",
      "Episode 25, Reward: 46.24729198716514\n",
      "Episode 26, Reward: 49.41207922499467\n",
      "Episode 27, Reward: 50.84892741513564\n",
      "Episode 28, Reward: 48.396077034519266\n",
      "Episode 29, Reward: 48.18941771069347\n",
      "Episode 30, Reward: 49.53181073929593\n",
      "Episode 31, Reward: 46.317491484429496\n",
      "Episode 32, Reward: 55.16526721703242\n",
      "Episode 33, Reward: 50.8755137358583\n",
      "Episode 34, Reward: 48.421150966279406\n",
      "Episode 35, Reward: 49.23782808490641\n",
      "Episode 36, Reward: 47.948209453075506\n",
      "Episode 37, Reward: 45.67698021208091\n",
      "Episode 38, Reward: 49.556749943169294\n",
      "Episode 39, Reward: 49.8901361960917\n",
      "Episode 40, Reward: 49.702183281003904\n",
      "Episode 41, Reward: 49.36504073421936\n",
      "Episode 42, Reward: 51.67605545013819\n",
      "Episode 43, Reward: 51.571773783058404\n",
      "Episode 44, Reward: 50.620753575765804\n",
      "Episode 45, Reward: 47.679682870516096\n",
      "Episode 46, Reward: 46.31901662024433\n",
      "Episode 47, Reward: 52.65636272889387\n",
      "Episode 48, Reward: 54.57045020762883\n",
      "Episode 49, Reward: 56.45223025526454\n",
      "Episode 50, Reward: 49.063241220418064\n",
      "Episode 51, Reward: 49.122183661980316\n",
      "Episode 52, Reward: 45.228376338133934\n",
      "Episode 53, Reward: 53.941114138282956\n",
      "Episode 54, Reward: 47.00132671620209\n",
      "Episode 55, Reward: 51.83023043921498\n",
      "Episode 56, Reward: 53.39159291383401\n",
      "Episode 57, Reward: 49.91132245507552\n",
      "Episode 58, Reward: 48.43041647458101\n",
      "Episode 59, Reward: 48.029932794581\n",
      "Episode 60, Reward: 53.94591971339571\n",
      "Episode 61, Reward: 52.4627848254143\n",
      "Episode 62, Reward: 54.489593703189904\n",
      "Episode 63, Reward: 50.42907035805393\n",
      "Episode 64, Reward: 52.789609347642646\n",
      "Episode 65, Reward: 49.07445352077277\n",
      "Episode 66, Reward: 48.81933896939174\n",
      "Episode 67, Reward: 49.490958054065146\n",
      "Episode 68, Reward: 49.836848860623824\n",
      "Episode 69, Reward: 47.663320474087996\n",
      "Episode 70, Reward: 50.24288910542801\n",
      "Episode 71, Reward: 50.57114168613017\n",
      "Episode 72, Reward: 48.881585440800556\n",
      "Episode 73, Reward: 48.15819680901538\n",
      "Episode 74, Reward: 45.99313221421906\n",
      "Episode 75, Reward: 46.04860373723614\n",
      "Episode 76, Reward: 56.13180457755933\n",
      "Episode 77, Reward: 52.467420704073604\n",
      "Episode 78, Reward: 48.525224108203645\n",
      "Episode 79, Reward: 49.36938126785063\n",
      "Episode 80, Reward: 50.48277486380645\n",
      "Episode 81, Reward: 48.66097102439197\n",
      "Episode 82, Reward: 52.45305853945779\n",
      "Episode 83, Reward: 51.01734519668378\n",
      "Episode 84, Reward: 50.31072681688253\n",
      "Episode 85, Reward: 46.591026470966675\n",
      "Episode 86, Reward: 47.29728860298967\n",
      "Episode 87, Reward: 49.162963409001065\n",
      "Episode 88, Reward: 48.09551092271142\n",
      "Episode 89, Reward: 47.49134742804211\n",
      "Episode 90, Reward: 49.554928731489554\n",
      "Episode 91, Reward: 47.44760758619979\n",
      "Episode 92, Reward: 47.28490908251395\n",
      "Episode 93, Reward: 52.60189838165441\n",
      "Episode 94, Reward: 50.95696056168083\n",
      "Episode 95, Reward: 51.12752265862686\n",
      "Episode 96, Reward: 52.23094372671696\n",
      "Episode 97, Reward: 48.78896035712968\n",
      "Episode 98, Reward: 50.61903762178792\n",
      "Episode 99, Reward: 54.19858738872557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# 定义深度 Q 网络模型\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 初始化环境和网络参数\n",
    "input_dim = 63  # 输入特征维度\n",
    "output_dim = 50  # 输出动作空间大小\n",
    "batch_size = 64\n",
    "gamma = 0.99  # 折扣因子\n",
    "epsilon = 0.1  # 探索率\n",
    "target_update = 10  # 目标网络更新频率\n",
    "memory_capacity = 10000\n",
    "lr = 0.001  # 学习率\n",
    "num_episodes = 100\n",
    "\n",
    "# 创建深度 Q 网络和目标网络\n",
    "policy_net = DQN(input_dim, output_dim)\n",
    "target_net = DQN(input_dim, output_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 初始化经验回放缓冲区\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "# 开始训练\n",
    "for episode in range(num_episodes):\n",
    "    state = torch.randn(input_dim)  # 初始化环境状态\n",
    "    episode_reward = 0\n",
    "    for t in range(100):  # 最大步数为100\n",
    "        # 选择动作\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(output_dim)  # 探索\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state)\n",
    "                action = q_values.argmax().item()  # 利用\n",
    "\n",
    "        # 与环境交互，获取下一个状态和奖励\n",
    "        next_state = torch.randn(input_dim)  # 假设下一个状态是随机的\n",
    "        reward = random.random()  # 假设奖励是随机的\n",
    "\n",
    "        # 存储经验样本到经验回放缓冲区\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 从经验回放缓冲区中抽样一批经验样本进行训练\n",
    "        if len(memory) > batch_size:\n",
    "            transitions = memory.sample(batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            state_batch = torch.stack(batch.state)\n",
    "            action_batch = torch.tensor(batch.action)\n",
    "            reward_batch = torch.tensor(batch.reward)\n",
    "            next_state_batch = torch.stack(batch.next_state)\n",
    "\n",
    "            # 计算目标 Q 值\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_state_batch)\n",
    "                target_q_values = reward_batch + gamma * next_q_values.max(1)[0]\n",
    "\n",
    "            # 计算损失\n",
    "            q_values = policy_net(state_batch)\n",
    "            loss = loss_fn(q_values.gather(1, action_batch.unsqueeze(1)), target_q_values.unsqueeze(1))\n",
    "\n",
    "            # 更新网络参数\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 更新目标网络参数\n",
    "        if t % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StepActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, knobs_dim):\n",
    "        super(StepActorCritic, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.actor_fc1 = nn.Linear(128, 128)\n",
    "        self.actor_fc2 = nn.Linear(128, 256)\n",
    "        self.actor_batchnorm = nn.BatchNorm1d(16)\n",
    "        self.actor_fc3 = nn.Linear(256, 128)\n",
    "        self.actor_output = nn.Linear(128, action_dim)\n",
    "\n",
    "        self.critic_fc1 = nn.Linear(128 + knobs_dim, 128)\n",
    "        self.critic_fc2 = nn.Linear(128, 256)\n",
    "        self.critic_batchnorm = nn.BatchNorm1d(16)\n",
    "        self.critic_fc3 = nn.Linear(256, 64)\n",
    "        self.critic_output = nn.Linear(64, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, state, knobs):\n",
    "        x = self.relu(self.input_layer(state))\n",
    "        actor_x = self.relu(self.actor_fc1(x))\n",
    "        actor_x = self.relu(self.actor_fc2(actor_x))\n",
    "        actor_x = self.actor_batchnorm(actor_x)\n",
    "        actor_x = self.relu(self.actor_fc3(actor_x))\n",
    "        actor_output = self.tanh(self.actor_output(actor_x))\n",
    "\n",
    "        critic_x = torch.cat([x, knobs], dim=1)\n",
    "        critic_x = self.relu(self.critic_fc1(critic_x))\n",
    "        critic_x = self.relu(self.critic_fc2(critic_x))\n",
    "        critic_x = self.critic_batchnorm(critic_x)\n",
    "        critic_x = self.relu(self.critic_fc3(critic_x))\n",
    "        critic_output = self.critic_output(critic_x)\n",
    "\n",
    "        return actor_output, critic_output\n",
    "\n",
    "# 创建 Step Actor-Critic 模型\n",
    "input_dim = 63  # 输入维度（状态维度）\n",
    "action_dim = 1  # 动作维度\n",
    "knobs_dim = 50  # Knobs 维度\n",
    "model = StepActorCritic(input_dim, action_dim, knobs_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [0.49774718 0.0238259  0.58595907]\n",
      "Action: [0.33093945 0.8780094 ]\n",
      "Input Vector: [0.49774718 0.0238259  0.58595907 0.33093945 0.8780094 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 示例中的状态空间大小和动作空间大小\n",
    "state_space_dim = 3  # 假设状态是一个3维的向量\n",
    "action_space_dim = 2  # 假设动作是一个2维的向量\n",
    "\n",
    "# 构造一个随机状态和动作\n",
    "state = np.random.rand(state_space_dim)\n",
    "action = np.random.rand(action_space_dim)\n",
    "\n",
    "# 将状态和动作组合成一个输入向量\n",
    "input_vector = np.concatenate([state, action])\n",
    "\n",
    "print(\"State:\", state)\n",
    "print(\"Action:\", action)\n",
    "print(\"Input Vector:\", input_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m--> 144\u001b[0m   action \u001b[38;5;241m=\u001b[39m ddpg\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m    145\u001b[0m   next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    146\u001b[0m   ddpg\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, done)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 定义Actor网络\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "    super(Actor, self).__init__()\n",
    "    self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.relu(self.fc1(state))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = self.tanh(self.fc3(x))\n",
    "    return x\n",
    "\n",
    "# 定义Critic网络\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    x = torch.cat([state, action], dim=1)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "# 定义经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.memory = []\n",
    "    self.position = 0\n",
    "\n",
    "  def push(self, state, action, reward, next_state, done):\n",
    "    if len(self.memory) < self.capacity:\n",
    "      self.memory.append(None)\n",
    "    self.memory[self.position] = (state, action, reward, next_state, done)\n",
    "    self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    batch = random.sample(self.memory, batch_size)\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    return (\n",
    "      torch.cat(state),\n",
    "      torch.cat(action),\n",
    "      torch.cat(reward),\n",
    "      torch.cat(next_state),\n",
    "      torch.cat(done)\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)\n",
    "\n",
    "# 定义DDPG算法\n",
    "class DDPG:\n",
    "  def __init__(self, state_dim, action_dim, hidden_dim, buffer_capacity, batch_size, gamma, tau):\n",
    "    self.actor = Actor(state_dim, action_dim, hidden_dim)\n",
    "    self.target_actor = Actor(state_dim, action_dim, hidden_dim)\n",
    "    self.critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "    self.target_critic = Critic(state_dim, action_dim, hidden_dim)\n",
    "    self.buffer = ReplayBuffer(buffer_capacity)\n",
    "    self.batch_size = batch_size\n",
    "    self.gamma = gamma\n",
    "    self.tau = tau\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action = self.actor(state).squeeze(0).detach().numpy()\n",
    "    return action\n",
    "\n",
    "  def update(self):\n",
    "    if len(self.buffer) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    state, action, reward, next_state, done = self.buffer.sample(self.batch_size)\n",
    "\n",
    "    # 更新Critic网络\n",
    "    target_actions = self.target_actor(next_state)\n",
    "    target_values = self.target_critic(next_state, target_actions)\n",
    "    target_q_values = reward + self.gamma * (1 - done) * target_values\n",
    "    q_values = self.critic(state, action)\n",
    "    critic_loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
    "\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    # 更新Actor网络\n",
    "    actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    # 更新目标网络\n",
    "    for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "  def store_transition(self, state, action, reward, next_state, done):\n",
    "    self.buffer.push(\n",
    "      torch.tensor(state, dtype=torch.float32),\n",
    "      torch.tensor(action, dtype=torch.float32),\n",
    "      torch.tensor(reward, dtype=torch.float32),\n",
    "      torch.tensor(next_state, dtype=torch.float32),\n",
    "      torch.tensor(done, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "# 定义环境和超参数\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "hidden_dim = 128\n",
    "buffer_capacity = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "max_episodes = 1000\n",
    "max_steps = 100\n",
    "\n",
    "# 创建DDPG对象\n",
    "ddpg = DDPG(state_dim, action_dim, hidden_dim, buffer_capacity, batch_size, gamma, tau)\n",
    "\n",
    "# 开始训练\n",
    "for episode in range(max_episodes):\n",
    "  # state = env.reset()\n",
    "  episode_reward = 0\n",
    "\n",
    "  for step in range(max_steps):\n",
    "    action = ddpg.select_action(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    ddpg.store_transition(state, action, reward, next_state, done)\n",
    "    ddpg.update()\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  print(f\"Episode {episode}: Reward = {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 离散输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc.weight', tensor([[-0.0377, -0.2629, -0.2633,  0.3115, -0.0655,  0.1771, -0.2742, -0.0703,\n",
      "         -0.0190,  0.1908],\n",
      "        [ 0.0665,  0.0934,  0.0943, -0.0377,  0.2301,  0.2442,  0.0438,  0.0271,\n",
      "          0.2959,  0.0316],\n",
      "        [ 0.0660, -0.2986, -0.0153, -0.2424,  0.2710,  0.1451, -0.1556,  0.3130,\n",
      "         -0.1683,  0.2102],\n",
      "        [-0.0674,  0.1161,  0.2509,  0.2573, -0.1128,  0.3116, -0.2782,  0.0519,\n",
      "         -0.0046, -0.0434],\n",
      "        [ 0.2822, -0.2520, -0.0802, -0.1769,  0.0259, -0.1637, -0.3048, -0.0032,\n",
      "          0.2323, -0.2376]])), ('fc.bias', tensor([ 0.1308,  0.0736, -0.2704,  0.1498, -0.0949]))])\n",
      "tensor([[-0.,  0., -0., -0., 10.],\n",
      "        [ 1.,  0., -1.,  1., 10.],\n",
      "        [ 1.,  0.,  2.,  1., 10.],\n",
      "        [ 1.,  0.,  1.,  0., 10.],\n",
      "        [ 1.,  0., -1.,  1., 10.],\n",
      "        [-0.,  0.,  1., -1., 10.],\n",
      "        [ 1.,  0., -1., -0., 10.],\n",
      "        [-0.,  0., -1.,  0., 10.],\n",
      "        [-0.,  0., -1.,  0., 10.],\n",
      "        [-0.,  0., -1., -0., 10.],\n",
      "        [ 0.,  0.,  0., -0., 10.],\n",
      "        [-0.,  0., -1., -0., 10.],\n",
      "        [ 0.,  0., -1.,  0., 10.],\n",
      "        [ 0.,  0., -1.,  0., 10.],\n",
      "        [-1.,  0., -1., -0., 10.],\n",
      "        [-1.,  1., -0., -1., 10.],\n",
      "        [ 1.,  0., -1., -0., 10.],\n",
      "        [ 2.,  0.,  0.,  0., 10.],\n",
      "        [-0.,  0.,  1., -0., 10.],\n",
      "        [-0.,  0., -0.,  0., 10.],\n",
      "        [ 1.,  0., -1.,  1., 10.],\n",
      "        [ 0.,  0., -0.,  0., 10.],\n",
      "        [ 0.,  0., -1.,  0., 10.],\n",
      "        [ 0.,  0., -0., -0., 10.],\n",
      "        [-2.,  0., -1., -1., 10.],\n",
      "        [-0.,  0.,  1., -0., 10.],\n",
      "        [ 1.,  0., -0.,  0., 10.],\n",
      "        [-0.,  0., -1., -0., 10.],\n",
      "        [-0.,  0.,  0., -0., 10.],\n",
      "        [ 0.,  0., -0., -0., 10.],\n",
      "        [ 1.,  0., -1.,  1., 10.],\n",
      "        [ 0.,  0., -1., -1., 10.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, output_ranges):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.output_ranges = output_ranges\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "      \n",
    "    def post_process(self, outputs):\n",
    "        processed_outputs = torch.zeros_like(outputs)  # 创建与输出相同形状的张量\n",
    "        for i, (min_val, max_val) in enumerate(self.output_ranges):\n",
    "            processed_outputs[:, i] = torch.round(torch.where(\n",
    "                outputs[:, i] < min_val,  # 如果输出小于最小值，取最小值\n",
    "                torch.tensor(min_val, dtype=outputs.dtype),\n",
    "                torch.where(\n",
    "                    outputs[:, i] > max_val,  # 如果输出大于最大值，取最大值\n",
    "                    torch.tensor(max_val, dtype=outputs.dtype),\n",
    "                    outputs[:, i]  # 否则保持不变\n",
    "                )\n",
    "            ))\n",
    "        return processed_outputs\n",
    "\n",
    "\n",
    "\n",
    "# 定义神经网络参数\n",
    "input_dim = 10\n",
    "output_dim = 5\n",
    "output_ranges = [(-10, 10), (0, 100), (-5, 5), (-20, 20), (10, 50)]  # 每个维度的最小和最大值\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# 创建模型\n",
    "model = MyModel(input_dim, output_dim, output_ranges)\n",
    "\n",
    "# 构造输入和标签数据（示例）\n",
    "inputs = torch.randn(batch_size, input_dim)\n",
    "labels = torch.randn(batch_size, output_dim)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # 后处理\n",
    "    outputs = model.post_process(outputs)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 输出训练结束后的模型参数\n",
    "print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "离散型参数的 Gumbel-Softmax 采样结果:\n",
      " tensor([[7.7437e-04, 1.9407e-02, 6.7812e-06, 6.3611e-07, 4.4379e-05, 1.4266e-04,\n",
      "         9.7250e-01, 1.6185e-06, 5.6145e-05, 3.4787e-07, 6.3270e-03, 2.4336e-07,\n",
      "         7.3745e-05, 3.7061e-05, 6.2972e-04],\n",
      "        [2.8606e-08, 1.1959e-10, 4.2689e-10, 5.3034e-07, 9.1048e-10, 1.4973e-09,\n",
      "         1.6246e-09, 5.7049e-10, 7.5383e-10, 3.1336e-09, 2.6383e-08, 2.4759e-09,\n",
      "         5.8222e-07, 1.0000e+00, 3.0148e-10],\n",
      "        [8.6851e-03, 4.4052e-06, 3.5064e-04, 7.9989e-01, 2.1584e-05, 3.8855e-03,\n",
      "         3.1125e-05, 6.9320e-03, 5.6447e-02, 2.5358e-04, 1.1837e-05, 1.7383e-02,\n",
      "         5.8447e-03, 9.6846e-02, 3.4093e-03],\n",
      "        [4.4312e-03, 1.9910e-05, 1.2040e-04, 1.0275e-01, 5.8551e-06, 9.3126e-05,\n",
      "         8.4422e-01, 9.8423e-03, 2.4273e-03, 8.7246e-04, 1.9203e-03, 2.6395e-05,\n",
      "         1.6867e-03, 3.0840e-02, 7.4415e-04],\n",
      "        [6.5097e-01, 4.9563e-05, 7.2133e-06, 2.6495e-02, 4.8437e-04, 1.8962e-01,\n",
      "         1.4640e-05, 2.4379e-02, 7.1997e-05, 8.7229e-04, 3.4488e-06, 6.8589e-02,\n",
      "         5.3948e-03, 3.0083e-04, 3.2751e-02],\n",
      "        [3.3955e-06, 1.6695e-04, 1.4053e-06, 3.1213e-05, 7.9225e-06, 3.7886e-01,\n",
      "         7.6629e-06, 2.7673e-05, 8.5890e-04, 1.5137e-03, 4.3921e-06, 1.7553e-04,\n",
      "         6.8765e-06, 9.7320e-04, 6.1736e-01],\n",
      "        [5.5768e-05, 2.5087e-03, 8.6713e-05, 7.5701e-05, 1.9445e-02, 1.5034e-03,\n",
      "         3.3211e-02, 4.7868e-02, 7.7957e-05, 1.3203e-02, 2.2079e-03, 8.5279e-01,\n",
      "         1.1613e-03, 7.8927e-05, 2.5725e-02],\n",
      "        [2.0794e-12, 3.5884e-11, 2.0195e-14, 4.9241e-09, 1.2156e-14, 8.7445e-15,\n",
      "         1.9695e-14, 1.0326e-14, 2.5223e-14, 2.1466e-14, 1.0000e+00, 2.5276e-15,\n",
      "         3.6525e-15, 2.2445e-13, 5.9503e-12],\n",
      "        [7.1927e-06, 1.4509e-07, 1.6475e-04, 4.3248e-04, 3.0280e-06, 4.3655e-07,\n",
      "         2.6926e-06, 8.8209e-04, 2.2743e-06, 3.1345e-05, 9.7392e-06, 1.9607e-06,\n",
      "         1.4918e-05, 9.9842e-01, 2.2900e-05],\n",
      "        [2.9952e-02, 4.5775e-03, 2.3988e-02, 5.3175e-03, 5.4137e-04, 1.7361e-01,\n",
      "         4.7180e-04, 6.0600e-01, 3.0396e-02, 9.8506e-07, 6.0429e-05, 8.1326e-02,\n",
      "         1.5882e-03, 4.2039e-02, 1.3345e-04],\n",
      "        [8.7455e-08, 1.2388e-03, 1.1292e-03, 6.3565e-06, 1.4530e-05, 1.3662e-05,\n",
      "         1.6371e-04, 4.7132e-06, 1.9885e-06, 7.7628e-05, 2.4140e-07, 9.9670e-01,\n",
      "         7.5758e-06, 5.9473e-04, 4.3899e-05],\n",
      "        [4.1000e-04, 5.6109e-01, 2.6986e-03, 9.7765e-05, 2.2950e-03, 2.8475e-06,\n",
      "         6.4574e-03, 8.6318e-06, 4.0863e-01, 3.0401e-03, 6.0348e-05, 1.4689e-02,\n",
      "         2.5541e-06, 5.2089e-05, 4.7072e-04],\n",
      "        [1.3676e-01, 2.9573e-03, 4.6274e-02, 2.7769e-01, 4.3008e-03, 1.1240e-04,\n",
      "         2.2777e-01, 1.5608e-04, 7.0765e-03, 5.1123e-02, 2.0660e-01, 4.7828e-04,\n",
      "         1.7670e-02, 2.8608e-03, 1.8180e-02],\n",
      "        [1.1467e-03, 5.9928e-05, 1.2964e-05, 3.0985e-01, 1.3988e-04, 6.1735e-03,\n",
      "         4.5157e-04, 1.1340e-03, 6.3945e-03, 6.4341e-02, 4.9884e-01, 7.5092e-04,\n",
      "         1.0411e-01, 5.4787e-03, 1.1124e-03],\n",
      "        [8.0183e-01, 1.0954e-04, 9.0761e-07, 5.3842e-04, 6.4664e-04, 1.6391e-01,\n",
      "         1.0449e-04, 1.7086e-03, 9.1469e-05, 5.1750e-06, 7.2832e-04, 2.2724e-05,\n",
      "         2.5148e-02, 3.1919e-05, 5.1184e-03],\n",
      "        [4.6459e-03, 1.1602e-01, 1.4029e-01, 5.6245e-02, 6.0861e-02, 1.0939e-04,\n",
      "         7.4351e-05, 2.5116e-01, 1.2506e-01, 1.9106e-03, 8.9261e-03, 1.7237e-01,\n",
      "         6.4132e-04, 6.1647e-02, 3.3401e-05],\n",
      "        [9.5766e-01, 3.6733e-05, 3.6941e-04, 6.9400e-04, 8.3122e-07, 1.2244e-03,\n",
      "         1.6684e-03, 2.1135e-04, 8.5724e-05, 1.5119e-02, 1.1040e-04, 3.3494e-05,\n",
      "         2.2140e-02, 4.2990e-06, 6.4036e-04],\n",
      "        [1.3822e-03, 3.1341e-04, 2.1054e-04, 1.5241e-01, 1.4711e-03, 2.3404e-05,\n",
      "         3.0240e-06, 3.0214e-04, 5.3457e-02, 1.5429e-04, 7.8574e-01, 2.0029e-06,\n",
      "         4.4969e-03, 2.7337e-05, 3.5644e-06],\n",
      "        [1.5696e-02, 1.0142e-02, 2.0840e-01, 1.5989e-02, 4.4140e-03, 4.3024e-01,\n",
      "         2.9260e-01, 5.3534e-03, 3.6905e-04, 1.2280e-04, 1.4218e-04, 4.5527e-03,\n",
      "         1.0669e-02, 1.3049e-03, 5.6912e-06],\n",
      "        [9.8550e-07, 2.4091e-05, 4.6490e-03, 9.8765e-06, 5.5225e-05, 5.3903e-05,\n",
      "         1.1710e-05, 5.3578e-06, 8.6851e-01, 3.5331e-07, 1.2569e-01, 3.9470e-05,\n",
      "         4.2934e-06, 9.3854e-04, 1.1284e-05],\n",
      "        [4.1893e-05, 8.2203e-01, 2.7399e-03, 3.3452e-04, 4.8757e-05, 1.0128e-03,\n",
      "         4.5556e-02, 1.0283e-04, 1.2203e-01, 5.0220e-05, 4.7222e-03, 9.0300e-05,\n",
      "         3.5891e-04, 6.0502e-04, 2.7675e-04],\n",
      "        [9.3618e-06, 5.2349e-05, 1.1333e-05, 2.1998e-05, 3.1780e-04, 6.4045e-06,\n",
      "         1.0529e-02, 2.5780e-05, 6.7497e-05, 4.8439e-04, 5.4140e-05, 1.5923e-04,\n",
      "         2.7396e-07, 9.8826e-01, 5.1541e-07],\n",
      "        [1.6465e-06, 9.1872e-07, 8.1529e-06, 4.8460e-04, 9.1713e-06, 1.2724e-01,\n",
      "         3.2609e-05, 4.6737e-02, 8.1048e-04, 7.6873e-03, 4.6247e-06, 1.6755e-05,\n",
      "         1.7089e-05, 8.1924e-03, 8.0876e-01],\n",
      "        [1.1163e-03, 2.6897e-05, 7.6056e-03, 3.0892e-05, 2.7623e-02, 3.0027e-04,\n",
      "         3.0749e-02, 1.9001e-05, 8.4181e-01, 1.4012e-04, 3.1975e-02, 5.8535e-02,\n",
      "         1.2762e-05, 5.1765e-05, 2.1164e-06],\n",
      "        [3.6066e-05, 5.6377e-06, 4.6878e-06, 2.7712e-03, 1.6990e-05, 7.6324e-04,\n",
      "         7.0049e-04, 2.4777e-04, 6.3156e-05, 2.9184e-06, 2.4975e-03, 1.0253e-06,\n",
      "         6.2117e-05, 9.9283e-01, 1.4159e-07],\n",
      "        [3.9018e-05, 5.0138e-06, 5.4308e-03, 4.8512e-04, 7.9177e-07, 9.9074e-01,\n",
      "         2.2492e-05, 2.2411e-06, 7.7468e-06, 1.2890e-04, 1.3009e-05, 3.0276e-03,\n",
      "         1.0549e-06, 8.9094e-05, 5.2168e-06],\n",
      "        [1.0376e-05, 1.0127e-06, 8.5736e-07, 1.2421e-05, 9.9962e-01, 2.5121e-06,\n",
      "         2.1971e-06, 1.0350e-07, 1.1785e-07, 1.3821e-07, 3.2586e-06, 1.2034e-06,\n",
      "         2.5107e-06, 3.3595e-04, 8.7780e-06],\n",
      "        [3.0908e-03, 1.1465e-04, 3.9317e-05, 8.5621e-03, 7.9802e-05, 3.0017e-05,\n",
      "         4.4885e-04, 7.6637e-05, 3.1020e-05, 1.3747e-03, 9.8398e-01, 2.3557e-04,\n",
      "         1.2364e-03, 6.7212e-04, 3.0448e-05],\n",
      "        [1.3307e-02, 3.0587e-03, 1.1370e-03, 3.1314e-02, 2.9648e-01, 6.0759e-04,\n",
      "         1.9453e-03, 1.5134e-03, 3.8938e-03, 3.6482e-04, 6.2100e-01, 2.6408e-03,\n",
      "         1.2271e-04, 2.1033e-02, 1.5788e-03],\n",
      "        [3.7014e-07, 1.3298e-06, 1.1097e-06, 1.1343e-03, 2.6669e-05, 6.7379e-03,\n",
      "         2.7178e-02, 1.3372e-03, 1.6865e-05, 7.3702e-04, 9.6257e-01, 1.1910e-05,\n",
      "         1.2069e-04, 9.8844e-05, 2.5200e-05],\n",
      "        [9.8275e-05, 3.6418e-04, 4.4373e-04, 9.0313e-06, 1.8125e-03, 1.5189e-07,\n",
      "         1.9616e-06, 3.8344e-05, 2.8325e-05, 2.3762e-03, 2.3263e-05, 2.2311e-04,\n",
      "         1.2964e-01, 1.1921e-05, 8.6493e-01],\n",
      "        [4.0864e-05, 1.0458e-01, 2.1402e-07, 8.0477e-06, 7.6289e-04, 3.5486e-03,\n",
      "         2.5212e-06, 3.9178e-08, 1.5653e-04, 6.3852e-07, 1.9661e-05, 1.2838e-05,\n",
      "         8.9083e-01, 2.9205e-05, 4.9971e-07]], grad_fn=<SoftmaxBackward0>)\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Gumbel\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_categories, temperature=1.0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        self.num_categories = num_categories\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        # 切分连续型参数和离散型参数\n",
    "        logits_continuous = logits[:, :self.num_categories]  # 连续型参数\n",
    "        logits_discrete = logits[:, self.num_categories:]   # 离散型参数\n",
    "\n",
    "        # 使用 Gumbel-Softmax 处理离散型参数\n",
    "        gumbel_dist = Gumbel(0, 1)\n",
    "        gumbel_noise = gumbel_dist.sample(logits_discrete.size()).to(x.device)\n",
    "        gumbel_discrete = F.gumbel_softmax(logits_discrete + gumbel_noise, self.temperature, dim=-1)\n",
    "\n",
    "        return logits_continuous, gumbel_discrete\n",
    "\n",
    "# 测试示例\n",
    "input_size = 10  # 输入维度\n",
    "output_size = 20  # 输出维度（包括连续和离散参数）\n",
    "num_categories = 5  # 离散参数的数量\n",
    "batch_size = 32  # 批大小\n",
    "temperature = 0.5  # Gumbel-Softmax 温度参数\n",
    "\n",
    "# 创建模型实例\n",
    "actor = Actor(input_size, output_size, num_categories, temperature)\n",
    "\n",
    "# 构造输入数据\n",
    "inputs = torch.randn(batch_size, input_size)\n",
    "\n",
    "# 前向传播\n",
    "logits_continuous, gumbel_discrete = actor(inputs)\n",
    "\n",
    "# 打印输出结果\n",
    "# print(\"连续型参数的 logits:\\n\", logits_continuous)\n",
    "print(\"离散型参数的 Gumbel-Softmax 采样结果:\\n\", gumbel_discrete)\n",
    "\n",
    "discrete_values = torch.argmax(gumbel_discrete, dim=-1)\n",
    "print(len(discrete_values))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
